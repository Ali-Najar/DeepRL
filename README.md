# ğŸ§  Deep Reinforcement Learning

This repository contains a series of tasks and Jupyter notebooks that guide you through the fundamentals and advanced topics of Deep Reinforcement Learning (DRL).  
Each part focuses on a specific area, building from foundational concepts to more complex methods.

---

## ğŸ“š Table of Contents

1.  [ğŸ“˜ Introduction to RL](#1-introduction-to-rl)
2. ğŸ“ˆ [Value-Based Methods](#2-value-based-methods)
3. ğŸ¯ [Policy-Based Methods](#3-policy-based-methods)
4. ğŸ§© [Advanced Methods](#4-advanced-methods)
5. ğŸ§  [Model-Based Methods](#5-model-based-methods)
6. ğŸ° [Multi-Armed Bandits](#6-multi-armed-bandits)
7. ğŸ”¢ [Value-Based Theory](#7-value-based-theory)
8. ğŸ§­ [Policy-Based Theory](#8-policy-based-theory)
9. ğŸ§¬ [Advanced Theory](#9-advanced-theory)
10. ğŸ•µï¸â€â™‚ï¸ [Exploration Methods](#10-exploration-methods)
11. ğŸ‘¥ [Imitation & Inverse RL](#11-imitation-and-inverse-rl)
12. ğŸ“¦ [Offline Methods](#12-offline-methods)
13. ğŸ¤– [Multi-Agent Methods](#13-multi-agent-methods)
14. ğŸ—ï¸ [Hierarchical & Meta RL](#14-hierarchical-and-meta-rl)

---

## ğŸ“˜ 1. Introduction to RL

In this section, we introduce the foundational principles of reinforcement learning through experiments on standard environments and a custom-designed Gym environment. The work includes detailed analyses of hyperparameters, learning curves, and performance metrics.


## ğŸ§¾ Contents

- ğŸŒ„ [Overview](#overview)
- ğŸ§ª [Experiments on Predefined Environments](#experiments-on-predefined-environments)
  - â„ï¸ [FrozenLake-v1](#frozenlake-v1)
  - ğŸ¡ [CartPole-v1](#cartpole-v1)
  - ğŸ“Š [DQN vs PPO: Quick Comparison](#dqn-vs-ppo-comparison)
- ğŸ¦– [Custom Environment: Chrome-Dino](#custom-environment-chrome-dino)
- ğŸ“¹ [Media](#media)
- ğŸ”— [References](#references)

## ğŸŒ„ Overview

This part of the project establishes the core concepts of Reinforcement Learning (RL) and evaluates two well-known environmentsâ€”FrozenLake-v1 and CartPole-v1â€”using both value-based (DQN) and policy-based (PPO) methods. In addition, a custom environment based on the Chrome-Dino game is implemented using Gymnasium and Pygame to showcase the application of RL in interactive settings.

## ğŸ§ª Experiments on Predefined Environments

### â„ï¸ FrozenLake-v1

In the FrozenLake-v1 environment, multiple models were trained using various hyperparameter settings. Key aspects of this experiment include:

- **Model Variants:**  
  - PPO and DQN were tested with different learning rates, discount factors, varying timesteps, and replay buffer sizes.
  - Additional experiments included the use of reward wrappers to analyze the impact of step costs.

- **Performance Analysis:**  
  - Learning curves were evaluated in terms of rewards per episode and mean episode lengths.
  - The results indicate that PPO generally exhibits smoother convergence, whereas DQN shows greater fluctuation and slower convergence.

### ğŸ¡ CartPole-v1

For the CartPole-v1 environment, the experiments focused on the following:

- **Hyperparameter Tuning:**  
  - Models were trained with different learning rates and discount factors, revealing that lower learning rates lead to more stable convergence.
  - Sufficient training timesteps were critical, with PPO requiring approximately 70,000 timesteps to achieve reliable performance.

- **Observations:**  
  - The continuous state dynamics of CartPole posed additional challenges, particularly for the DQN approach.
 
### ğŸ“Š DQN vs PPO: Quick Comparison

<div align="center">
  
| Environment     | Algorithm | Convergence Speed | Stability | Final Performance |
|----------------|-----------|-------------------|-----------|-------------------|
| FrozenLake-v1   | DQN       | Slow              | Low       | Medium            |
|                 | PPO       | Fast              | High      | High              |
| CartPole-v1     | DQN       | Medium            | Moderate  | High              |
|                 | PPO       | Fast              | High      | High              |

</div>

<p align="center">
  <img src="./01_Introduction to RL/frozen-lake.png" alt="DQN vs PPO on FrozenLake-v1" width="600"/>
  <br/>
  <em>Comparison of DQN (ğŸŸ£) and PPO (âš«) on FrozenLake-v1.</em>
</p>

<p align="center">
  <img src="./01_Introduction to RL/cartpole.png" alt="DQN vs PPO on CartPole-v1" width="300"/>
  <br/>
  <em>Comparison of DQN (ğŸŸ£) and PPO (âš«) on CartPole-v1.</em>
</p>

## ğŸ¦– Custom Environment: Chrome-Dino

A custom Gymnasium environment was developed to simulate the Chrome-Dino game using Pygame. This environment is defined by:

- **State Space:**  
  - A 3-dimensional vector including:
    - `dino_y`: Vertical position (range: 0â€“400)
    - `dino_vel_y`: Vertical velocity (range: â€“20 to 20)
    - `closest_dist`: Distance to the nearest obstacle (range: 0â€“1000)

- **Action Space:**  
  - Discrete actions: `0` (No Operation), `1` (Jump), and `2` (Duck).

- **Reward Function:**  
  - +1 per timestep for survival,
  - +5 for successfully passing an obstacle,
  - -200 upon collision (which terminates the episode).

The agent was trained using PPO with an MLP policy over 2,000,000 timesteps. This setup demonstrates how reinforcement learning techniques can be applied to interactive environments with dynamic visual feedback.

## ğŸ“¹ Media

Below is the Chrome-Dino test video.

<p align="center">
  <img src="01_Introduction to RL/output.gif" alt="Chrome Dino PPO Agent Demo" width="300"/>
  <br/>
  <em>PPO agent running in the custom Chrome-Dino environment.</em>
</p>
